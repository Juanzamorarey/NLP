{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import everything that we need and create a list with the files that we're gonna use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "    'otro/edgar_allan_poe.txt',\n",
    "    'otro/robert_frost.txt'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought in here we will be facing a task of unsupervised learning we turn it into a supervised learning by storing into two lists the lines of the dataset and its labels. There are important things to be mentioned here. We are not using 2 different datasets, we are usign both of the authors in a unique dataset and from there making the differentiation between training and testing. So in this following step we do a bunch of things:\n",
    "\n",
    "-in our lists we store for each file, the label for it (all the lines in poe will have a 0 and all the lines of frost a 1. So our list labels will contain [0,0,0...1,1,1,] one for each line in the files.)\n",
    "\n",
    "-We're normalizing the text in order to reduce dimensionality, which means that we're erasing turning of words to lowercase as well as eliminating the endlines at the end of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otro/edgar_allan_poe.txt corresponds to label 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'otro/edgar_allan_poe.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juan_\\Desktop\\programación\\NLP_ML_DL_UdemyCourse\\Statistics models\\Text_classifier_markov_definitive.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juan_/Desktop/programaci%C3%B3n/NLP_ML_DL_UdemyCourse/Statistics%20models/Text_classifier_markov_definitive.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m label, document \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(input_files):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juan_/Desktop/programaci%C3%B3n/NLP_ML_DL_UdemyCourse/Statistics%20models/Text_classifier_markov_definitive.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdocument\u001b[39m}\u001b[39;00m\u001b[39m corresponds to label \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juan_/Desktop/programaci%C3%B3n/NLP_ML_DL_UdemyCourse/Statistics%20models/Text_classifier_markov_definitive.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39;49m(document):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juan_/Desktop/programaci%C3%B3n/NLP_ML_DL_UdemyCourse/Statistics%20models/Text_classifier_markov_definitive.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mrstrip()\u001b[39m.\u001b[39mlower()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juan_/Desktop/programaci%C3%B3n/NLP_ML_DL_UdemyCourse/Statistics%20models/Text_classifier_markov_definitive.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39mif\u001b[39;00m line:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'otro/edgar_allan_poe.txt'"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "labels = []\n",
    "\n",
    "for label, document in enumerate(input_files):\n",
    "    print(f\"{document} corresponds to label {label}\")\n",
    "    for line in open(document):\n",
    "        line = line.rstrip().lower()\n",
    "        if line:\n",
    "            line = line.translate(str.maketrans('','',string.punctuation))\n",
    "            input_texts.append(line)\n",
    "            labels.append(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done we use sklearn to make the differentiation between train and test. The variables with text will contain the lines and the Yvariables will contain the labels. Remember that, althought they are in different lists, we keep track of which line corresponds to which label by matching the in the index of the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, Ytrain, Ytest = train_test_split(input_texts,labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create an index variable and a dictionary in order to make the word to index mapping. Each word would be a number in our word to index mapping so, in order to be aware of the unknown words that can appear in the test set, we create a first state for the model with the index 0, and after that we create our dictionary of word to index mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = 1\n",
    "word_to_index_mapping = {'<unknown>':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in train_text:\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if word not in word_to_index_mapping:\n",
    "            word_to_index_mapping[word]=indice\n",
    "            indice+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unknown>': 0,\n",
       " 'whose': 1,\n",
       " 'entablatures': 2,\n",
       " 'intertwine': 3,\n",
       " 'lying': 4,\n",
       " 'down': 5,\n",
       " 'to': 6,\n",
       " 'die': 7,\n",
       " 'have': 8,\n",
       " 'suddenly': 9,\n",
       " 'arisen': 10,\n",
       " 'and': 11,\n",
       " 'both': 12,\n",
       " 'that': 13,\n",
       " 'morning': 14,\n",
       " 'equally': 15,\n",
       " 'lay': 16,\n",
       " 'politician': 17,\n",
       " 'at': 18,\n",
       " 'odd': 19,\n",
       " 'seasons': 20,\n",
       " 'i': 21,\n",
       " 'saw': 22,\n",
       " 'but': 23,\n",
       " 'them': 24,\n",
       " 'only': 25,\n",
       " 'for': 26,\n",
       " 'hours': 27,\n",
       " 'perhaps': 28,\n",
       " 'hear': 29,\n",
       " 'some': 30,\n",
       " 'word': 31,\n",
       " 'about': 32,\n",
       " 'the': 33,\n",
       " 'weather': 34,\n",
       " 'a': 35,\n",
       " 'winter': 36,\n",
       " 'garden': 37,\n",
       " 'in': 38,\n",
       " 'an': 39,\n",
       " 'alder': 40,\n",
       " 'swamp': 41,\n",
       " 'such': 42,\n",
       " 'as': 43,\n",
       " 'it': 44,\n",
       " 'is': 45,\n",
       " 'isnt': 46,\n",
       " 'worth': 47,\n",
       " 'mortgage': 48,\n",
       " 'who': 49,\n",
       " 'has': 50,\n",
       " 'heart': 51,\n",
       " 'your': 52,\n",
       " 'getting': 53,\n",
       " 'lost': 54,\n",
       " 'hadnt': 55,\n",
       " 'you': 56,\n",
       " 'long': 57,\n",
       " 'suspected': 58,\n",
       " 'where': 59,\n",
       " 'were': 60,\n",
       " 'love': 61,\n",
       " 'of': 62,\n",
       " 'years': 63,\n",
       " 'till': 64,\n",
       " 'they': 65,\n",
       " 'sorrowfully': 66,\n",
       " 'trailed': 67,\n",
       " 'dust': 68,\n",
       " 'made': 69,\n",
       " 'him': 70,\n",
       " 'throw': 71,\n",
       " 'his': 72,\n",
       " 'bare': 73,\n",
       " 'legs': 74,\n",
       " 'out': 75,\n",
       " 'bed': 76,\n",
       " 'this': 77,\n",
       " 'ghoulhaunted': 78,\n",
       " 'woodland': 79,\n",
       " 'weir': 80,\n",
       " 'playthings': 81,\n",
       " 'playhouse': 82,\n",
       " 'children': 83,\n",
       " 'maria': 84,\n",
       " 'thou': 85,\n",
       " 'hast': 86,\n",
       " 'heard': 87,\n",
       " 'my': 88,\n",
       " 'hymn': 89,\n",
       " 'entertain': 90,\n",
       " 'birds': 91,\n",
       " 'hold': 92,\n",
       " 'flowers': 93,\n",
       " 'point': 94,\n",
       " 'us': 95,\n",
       " 'path': 96,\n",
       " 'skies': 97,\n",
       " 'used': 98,\n",
       " 'live': 99,\n",
       " 'here': 100,\n",
       " 'robinsons': 101,\n",
       " 'see': 102,\n",
       " 'all': 103,\n",
       " 'money': 104,\n",
       " 'goes': 105,\n",
       " 'so': 106,\n",
       " 'fast': 107,\n",
       " 'was': 108,\n",
       " 'way': 109,\n",
       " 'he': 110,\n",
       " 'started': 111,\n",
       " 'courting': 112,\n",
       " 'me': 113,\n",
       " 'mother': 114,\n",
       " 'we': 115,\n",
       " 'know': 116,\n",
       " 'had': 117,\n",
       " 'grave': 118,\n",
       " 'cellar': 119,\n",
       " 'then': 120,\n",
       " 'went': 121,\n",
       " 'round': 122,\n",
       " 'on': 123,\n",
       " 'feet': 124,\n",
       " 'couldnt': 125,\n",
       " 'climb': 126,\n",
       " 'slippery': 127,\n",
       " 'slope': 128,\n",
       " 'bells': 129,\n",
       " 'ah': 130,\n",
       " 'melancholy': 131,\n",
       " 'waters': 132,\n",
       " 'lie': 133,\n",
       " 'be': 134,\n",
       " 'dragged': 135,\n",
       " 'by': 136,\n",
       " 'over': 137,\n",
       " 'everywhere': 138,\n",
       " 'never': 139,\n",
       " 'room': 140,\n",
       " 'before': 141,\n",
       " 'show': 142,\n",
       " 'thy': 143,\n",
       " 'grief': 144,\n",
       " 'joy': 145,\n",
       " 'hate': 146,\n",
       " 'many': 147,\n",
       " 'must': 148,\n",
       " 'seen': 149,\n",
       " 'make': 150,\n",
       " 'chisel': 151,\n",
       " 'work': 152,\n",
       " 'enormous': 153,\n",
       " 'glacier': 154,\n",
       " 'half': 155,\n",
       " 'boring': 156,\n",
       " 'through': 157,\n",
       " 'climbing': 158,\n",
       " 'blow': 159,\n",
       " 'earth': 160,\n",
       " 'or': 161,\n",
       " 'anything': 162,\n",
       " 'selfclear': 163,\n",
       " 'ive': 164,\n",
       " 'listened': 165,\n",
       " 'among': 166,\n",
       " 'sounds': 167,\n",
       " 'fever': 168,\n",
       " 'moonbeam': 169,\n",
       " 'hangs': 170,\n",
       " 'oer': 171,\n",
       " 'she': 172,\n",
       " 'tears': 173,\n",
       " 'are': 174,\n",
       " 'not': 175,\n",
       " 'dry': 176,\n",
       " 'fight': 177,\n",
       " 'smother': 178,\n",
       " 'when': 179,\n",
       " 'young': 180,\n",
       " 'roar': 181,\n",
       " 'softmurmured': 182,\n",
       " 'words': 183,\n",
       " 'let': 184,\n",
       " 'there': 185,\n",
       " 'light': 186,\n",
       " 'either': 187,\n",
       " 'looking': 188,\n",
       " 'finding': 189,\n",
       " 'something': 190,\n",
       " 'hard': 191,\n",
       " 'dim': 192,\n",
       " 'lake': 193,\n",
       " 'auber': 194,\n",
       " 'up': 195,\n",
       " 'fanes': 196,\n",
       " 'babylonlike': 197,\n",
       " 'walls': 198,\n",
       " 'how': 199,\n",
       " 'should': 200,\n",
       " 'thee': 201,\n",
       " 'deem': 202,\n",
       " 'wise': 203,\n",
       " 'like': 204,\n",
       " 'too': 205,\n",
       " 'makes': 206,\n",
       " 'worse': 207,\n",
       " 'silent': 208,\n",
       " 'streams': 209,\n",
       " 'sometimes': 210,\n",
       " 'seemed': 211,\n",
       " 'yield': 212,\n",
       " 'arent': 213,\n",
       " 'afraid': 214,\n",
       " 'whats': 215,\n",
       " 'gun': 216,\n",
       " 'son': 217,\n",
       " 'headboard': 218,\n",
       " 'mothers': 219,\n",
       " 'pushed': 220,\n",
       " 'summoning': 221,\n",
       " 'spirits': 222,\n",
       " 'button': 223,\n",
       " 'singing': 224,\n",
       " 'sweet': 225,\n",
       " 'swift': 226,\n",
       " 'with': 227,\n",
       " 'clouds': 228,\n",
       " 'low': 229,\n",
       " 'trailing': 230,\n",
       " 'moments': 231,\n",
       " 'rain': 232,\n",
       " 'misted': 233,\n",
       " 'attic': 234,\n",
       " 'might': 235,\n",
       " 'still': 236,\n",
       " 'cousins': 237,\n",
       " 'spoke': 238,\n",
       " 'clock': 239,\n",
       " 'judged': 240,\n",
       " 'crystal': 241,\n",
       " 'chill': 242,\n",
       " 'becoming': 243,\n",
       " 'reconciled': 244,\n",
       " 'wasted': 245,\n",
       " 'snow': 246,\n",
       " 'wings': 247,\n",
       " 'until': 248,\n",
       " 'can': 249,\n",
       " 'what': 250,\n",
       " 'troubling': 251,\n",
       " 'granny': 252,\n",
       " 'though': 253,\n",
       " 'often': 254,\n",
       " 'forget': 255,\n",
       " 'time': 256,\n",
       " 'lone': 257,\n",
       " 'dont': 258,\n",
       " 'very': 259,\n",
       " 'well': 260,\n",
       " 'help': 261,\n",
       " 'tempted': 262,\n",
       " 'her': 263,\n",
       " 'gloom': 264,\n",
       " 'shattered': 265,\n",
       " 'dishes': 266,\n",
       " 'underneath': 267,\n",
       " 'pine': 268,\n",
       " 'sacred': 269,\n",
       " 'sun': 270,\n",
       " 'weeping': 271,\n",
       " 'bless': 272,\n",
       " 'besides': 273,\n",
       " 'wear': 274,\n",
       " 'iron': 275,\n",
       " 'wagon': 276,\n",
       " 'wheels': 277,\n",
       " 'found': 278,\n",
       " 'hed': 279,\n",
       " 'gnawed': 280,\n",
       " 'four': 281,\n",
       " 'posts': 282,\n",
       " 'leaves': 283,\n",
       " 'crisped': 284,\n",
       " 'sere': 285,\n",
       " 'ecstasies': 286,\n",
       " 'above': 287,\n",
       " 'demon': 288,\n",
       " 'no': 289,\n",
       " 'its': 290,\n",
       " 'misting': 291,\n",
       " 'lets': 292,\n",
       " 'fair': 293,\n",
       " 'john': 294,\n",
       " 'likes': 295,\n",
       " 'tell': 296,\n",
       " 'offers': 297,\n",
       " 'estelle': 298,\n",
       " 'ready': 299,\n",
       " 'thought': 300,\n",
       " 'almost': 301,\n",
       " 'burden': 302,\n",
       " 'body': 303,\n",
       " 'song': 304,\n",
       " 'take': 305,\n",
       " 'nor': 306,\n",
       " 'will': 307,\n",
       " 'thinking': 308,\n",
       " 'now': 309,\n",
       " 'air': 310,\n",
       " 'ride': 311,\n",
       " 'lethean': 312,\n",
       " 'peace': 313,\n",
       " 'wont': 314,\n",
       " 'burned': 315,\n",
       " 'stake': 316,\n",
       " 'moving': 317,\n",
       " 'flock': 318,\n",
       " 'hens': 319,\n",
       " 'from': 320,\n",
       " 'place': 321,\n",
       " 'every': 322,\n",
       " 'lifelike': 323,\n",
       " 'posture': 324,\n",
       " 'swarm': 325,\n",
       " 'brook': 326,\n",
       " 'raises': 327,\n",
       " 'empty': 328,\n",
       " 'valley': 329,\n",
       " 'while': 330,\n",
       " 'invested': 331,\n",
       " 'reasons': 332,\n",
       " 'owe': 333,\n",
       " 'most': 334,\n",
       " 'gratitude': 335,\n",
       " 'am': 336,\n",
       " 'guide': 337,\n",
       " 'rests': 338,\n",
       " 'always': 339,\n",
       " 'cut': 340,\n",
       " 'off': 341,\n",
       " 'seraph': 342,\n",
       " 'spread': 343,\n",
       " 'pinion': 344,\n",
       " 'thence': 345,\n",
       " 'sprung': 346,\n",
       " 'numerous': 347,\n",
       " 'tribe': 348,\n",
       " 'upon': 349,\n",
       " 'longer': 350,\n",
       " 'axis': 351,\n",
       " 'nearest': 352,\n",
       " 'resembles': 353,\n",
       " 'worship': 354,\n",
       " 'oh': 355,\n",
       " 'remember': 356,\n",
       " 'shouldnt': 357,\n",
       " 'youre': 358,\n",
       " 'times': 359,\n",
       " 'arthur': 360,\n",
       " 'amy': 361,\n",
       " 'struck': 362,\n",
       " 'hand': 363,\n",
       " 'brittle': 364,\n",
       " 'floor': 365,\n",
       " 'birch': 366,\n",
       " 'boughs': 367,\n",
       " 'enough': 368,\n",
       " 'piled': 369,\n",
       " 'everywhere—': 370,\n",
       " 'strapping': 371,\n",
       " 'girl': 372,\n",
       " 'twenty': 373,\n",
       " 'ranged': 374,\n",
       " 'spirit': 375,\n",
       " 'communing': 376,\n",
       " 'angels': 377,\n",
       " 'watch': 378,\n",
       " 'woods': 379,\n",
       " 'fill': 380,\n",
       " 'thats': 381,\n",
       " 'man': 382,\n",
       " 'do': 383,\n",
       " 'age': 384,\n",
       " 'been': 385,\n",
       " 'cutting': 386,\n",
       " 'trees': 387,\n",
       " 'which': 388,\n",
       " 'sends': 389,\n",
       " 'runs': 390,\n",
       " 'unbridled': 391,\n",
       " 'course': 392,\n",
       " 'stopping': 393,\n",
       " 'tamed': 394,\n",
       " 'primeval': 395,\n",
       " 'wood': 396,\n",
       " 'smell': 397,\n",
       " 'fire': 398,\n",
       " 'drowned': 399,\n",
       " 'chandelier': 400,\n",
       " 'lips': 401,\n",
       " 'melody': 402,\n",
       " 'raspberries': 403,\n",
       " 'hew': 404,\n",
       " 'shape': 405,\n",
       " 'conquered': 406,\n",
       " 'scruples': 407,\n",
       " 'mountain': 408,\n",
       " 'may': 409,\n",
       " 'shifted': 410,\n",
       " 'since': 411,\n",
       " 'began': 412,\n",
       " 'mask': 413,\n",
       " 'viol': 414,\n",
       " 'vine': 415,\n",
       " 'present': 416,\n",
       " 'pathway': 417,\n",
       " 'part': 418,\n",
       " 'winged': 419,\n",
       " 'odour': 420,\n",
       " 'away': 421,\n",
       " 'come': 422,\n",
       " 'despite': 423,\n",
       " 'lion': 424,\n",
       " 'twothousandmile': 425,\n",
       " 'coast': 426,\n",
       " 'gone': 427,\n",
       " 'their': 428,\n",
       " 'eternal': 429,\n",
       " 'rest': 430,\n",
       " 'trembled': 431,\n",
       " 'stirred': 432,\n",
       " 'fireweed': 433,\n",
       " 'loving': 434,\n",
       " 'burnt': 435,\n",
       " 'didst': 436,\n",
       " 'glide': 437,\n",
       " 'thine': 438,\n",
       " 'eyes': 439,\n",
       " 'remained': 440,\n",
       " 'estelles': 441,\n",
       " 'shes': 442,\n",
       " 'settled': 443,\n",
       " 'gailyjewelld': 444,\n",
       " 'dead': 445,\n",
       " 'death': 446,\n",
       " 'looks': 447,\n",
       " 'gigantically': 448,\n",
       " 'against': 449,\n",
       " 'door': 450,\n",
       " 'nailed': 451,\n",
       " 'arose': 452,\n",
       " 'duplicate': 453,\n",
       " 'horn': 454,\n",
       " 'than': 455,\n",
       " 'folks': 456,\n",
       " 'any': 457,\n",
       " 'business': 458,\n",
       " 'stop': 459,\n",
       " 'lift': 460,\n",
       " 'face': 461,\n",
       " 'go': 462,\n",
       " 'horrible': 463,\n",
       " 'monody': 464,\n",
       " 'floats': 465,\n",
       " 'liked': 466,\n",
       " 'everything': 467,\n",
       " 'donkeys': 468,\n",
       " 'ears': 469,\n",
       " 'suggest': 470,\n",
       " 'shake': 471,\n",
       " 'our': 472,\n",
       " 'own': 473,\n",
       " 'stroked': 474,\n",
       " 'hornyhanded': 475,\n",
       " 'kindness': 476,\n",
       " 'well—i—be—': 477,\n",
       " 'said': 478,\n",
       " 'eddy': 479,\n",
       " 'toppling': 480,\n",
       " 'weak': 481,\n",
       " 'ye': 482,\n",
       " 'deliverers': 483,\n",
       " 'athens': 484,\n",
       " 'shame': 485,\n",
       " 'would': 486,\n",
       " 'suffice': 487,\n",
       " 'scoriac': 488,\n",
       " 'rivers': 489,\n",
       " 'roll': 490,\n",
       " 'wed': 491,\n",
       " 'ever': 492,\n",
       " 'want': 493,\n",
       " 'again': 494,\n",
       " 'truth': 495,\n",
       " 'speak': 496,\n",
       " 'voices': 497,\n",
       " 'utmost': 498,\n",
       " 'star': 499,\n",
       " 'today': 500,\n",
       " 'soul': 501,\n",
       " 'least': 502,\n",
       " 'solace': 503,\n",
       " 'hath': 504,\n",
       " 'dew': 505,\n",
       " 'night': 506,\n",
       " 'summer': 507,\n",
       " 'grass': 508,\n",
       " 'heaven': 509,\n",
       " 'gives': 510,\n",
       " 'glimpses': 511,\n",
       " 'those': 512,\n",
       " 'set': 513,\n",
       " 'far': 514,\n",
       " 'even': 515,\n",
       " 'meridian': 516,\n",
       " 'glare': 517,\n",
       " 'day': 518,\n",
       " 'havent': 519,\n",
       " 'strange': 520,\n",
       " 'house': 521,\n",
       " 'more': 522,\n",
       " 'why': 523,\n",
       " 'married': 524,\n",
       " 'hole': 525,\n",
       " 'wonder': 526,\n",
       " 'whole': 527,\n",
       " 'edition': 528,\n",
       " 'packing': 529,\n",
       " 'case': 530,\n",
       " 'one': 531,\n",
       " 'another': 532,\n",
       " 'thus': 533,\n",
       " 'pacified': 534,\n",
       " 'psyche': 535,\n",
       " 'kissed': 536,\n",
       " 'road': 537,\n",
       " 'didnt': 538,\n",
       " 'recognize': 539,\n",
       " 'lifted': 540,\n",
       " 'rug': 541,\n",
       " 'bred': 542,\n",
       " 'replied': 543,\n",
       " 'ulalume': 544,\n",
       " 'stardials': 545,\n",
       " 'pointed': 546,\n",
       " 'morn': 547,\n",
       " 'revolving': 548,\n",
       " 'scene': 549,\n",
       " 'could': 550,\n",
       " 'souls': 551,\n",
       " 'someone': 552,\n",
       " 'came': 553,\n",
       " 'cart': 554,\n",
       " 'pair': 555,\n",
       " 'discordant': 556,\n",
       " 'breathing': 557,\n",
       " 'trust': 558,\n",
       " 'youve': 559,\n",
       " 'upturnd': 560,\n",
       " 'alas': 561,\n",
       " 'sorrow': 562,\n",
       " 'got': 563,\n",
       " 'wild': 564,\n",
       " 'backs': 565,\n",
       " 'knew': 566,\n",
       " 'minute': 567,\n",
       " 'hoary': 568,\n",
       " 'groans': 569,\n",
       " 'woe': 570,\n",
       " 'done': 571,\n",
       " 'braced': 572,\n",
       " 'arctic': 573,\n",
       " 'pole': 574,\n",
       " 'acquaintance': 575,\n",
       " 'adventurously': 576,\n",
       " 'faith': 577,\n",
       " 'godliness': 578,\n",
       " 'throne': 579,\n",
       " 'save': 580,\n",
       " 'shall': 581,\n",
       " 'meet': 582,\n",
       " 'name': 583,\n",
       " 'hour': 584,\n",
       " 'deep': 585,\n",
       " 'hope': 586,\n",
       " 'died': 587,\n",
       " 'stairs': 588,\n",
       " 'two': 589,\n",
       " 'footsteps': 590,\n",
       " 'each': 591,\n",
       " 'step': 592,\n",
       " 'ghost—': 593,\n",
       " 'west': 594,\n",
       " 'gold': 595,\n",
       " 'gems': 596,\n",
       " 'turn': 597,\n",
       " 'back': 598,\n",
       " 'into': 599,\n",
       " 'driven': 600,\n",
       " 'wet': 601,\n",
       " 'swollen': 602,\n",
       " 'withering': 603,\n",
       " 'became': 604,\n",
       " 'man—': 605,\n",
       " 'terror': 606,\n",
       " 'letting': 607,\n",
       " 'sink': 608,\n",
       " 'wearing': 609,\n",
       " 'feeling': 610,\n",
       " 'crown': 611,\n",
       " 'memories': 612,\n",
       " 'radiant': 613,\n",
       " 'unless': 614,\n",
       " 'early': 615,\n",
       " 'leafs': 616,\n",
       " 'flower': 617,\n",
       " 'plain': 618,\n",
       " 'statement': 619,\n",
       " 'relationship': 620,\n",
       " 'being': 621,\n",
       " 'art': 622,\n",
       " 'windows': 623,\n",
       " 'banked': 624,\n",
       " 'sawdust': 625,\n",
       " 'sing': 626,\n",
       " 'embalmed': 627,\n",
       " 'echoing': 628,\n",
       " 'songs': 629,\n",
       " 'does': 630,\n",
       " 'look': 631,\n",
       " 'stay': 632,\n",
       " 'drawn': 633,\n",
       " 'hearts': 634,\n",
       " 'passion': 635,\n",
       " 'tone': 636,\n",
       " 'remembered': 637,\n",
       " 'dank': 638,\n",
       " 'tarn': 639,\n",
       " 'doll': 640,\n",
       " 'pasture': 641,\n",
       " 'rig': 642,\n",
       " 'pen': 643,\n",
       " 'falls': 644,\n",
       " 'powerless': 645,\n",
       " 'shivering': 646,\n",
       " 'rockstrewn': 647,\n",
       " 'town': 648,\n",
       " 'farming': 649,\n",
       " 'fallen': 650,\n",
       " 'old': 651,\n",
       " 'byroad': 652,\n",
       " 'left': 653,\n",
       " 'forty': 654,\n",
       " 'ago': 655,\n",
       " 'high': 656,\n",
       " 'gentle': 657,\n",
       " 'cast': 658,\n",
       " 'stark': 659,\n",
       " 'family': 660,\n",
       " 'im': 661,\n",
       " 'member': 662,\n",
       " 'shut': 663,\n",
       " 'ill': 664,\n",
       " 'stock': 665,\n",
       " 'village': 666,\n",
       " 'library': 667,\n",
       " 'fervor': 668,\n",
       " 'power': 669,\n",
       " 'darkest': 670,\n",
       " 'evening': 671,\n",
       " 'year': 672,\n",
       " 'planets': 673,\n",
       " 'rapid': 674,\n",
       " 'pleiads': 675,\n",
       " 'yes': 676,\n",
       " 'cant': 677,\n",
       " 'bought': 678,\n",
       " 'paid': 679,\n",
       " 'bill': 680,\n",
       " 'beads': 681,\n",
       " 'strains': 682,\n",
       " 'much': 683,\n",
       " 'these': 684,\n",
       " 'told': 685,\n",
       " 'tutelar': 686,\n",
       " 'shrine': 687,\n",
       " 'front': 688,\n",
       " 'hall': 689,\n",
       " 'id': 690,\n",
       " 'hitched': 691,\n",
       " 'plan': 692,\n",
       " 'gaining': 693,\n",
       " 'wealth': 694,\n",
       " 'tree': 695,\n",
       " 'raised': 696,\n",
       " 'again—': 697,\n",
       " 'daylight': 698,\n",
       " 'beauty': 699,\n",
       " 'birth': 700,\n",
       " 'home': 701,\n",
       " 'glory': 702,\n",
       " 'ask': 703,\n",
       " 'if': 704,\n",
       " 'mistake': 705,\n",
       " 'baubles': 706,\n",
       " 'get': 707,\n",
       " 'bring': 708,\n",
       " 'chalkpile': 709,\n",
       " 'going': 710,\n",
       " 'jupiter': 711,\n",
       " 'fell': 712,\n",
       " 'stars': 713,\n",
       " 'whence': 714,\n",
       " 'forth': 715,\n",
       " 'odorous': 716,\n",
       " 'ecstatic': 717,\n",
       " 'level': 718,\n",
       " 'nearer': 719,\n",
       " 'overhead': 720,\n",
       " 'scarce': 721,\n",
       " 'prize': 722,\n",
       " 'behind': 723,\n",
       " 'hillside': 724,\n",
       " 'other': 725,\n",
       " 'listening': 726,\n",
       " 'things': 727,\n",
       " 'lantern': 728,\n",
       " 'drop': 729,\n",
       " 'lot': 730,\n",
       " 'record': 731,\n",
       " 'trodden': 732,\n",
       " 'black': 733,\n",
       " 'little': 734,\n",
       " 'tin': 735,\n",
       " 'box': 736,\n",
       " 'cupboard': 737,\n",
       " 'shelf': 738,\n",
       " 'sincere': 739,\n",
       " 'reply': 740,\n",
       " 'nights': 741,\n",
       " 'mean': 742,\n",
       " 'harm': 743,\n",
       " 'herself': 744,\n",
       " 'tender': 745,\n",
       " 'verse': 746,\n",
       " 'escaped': 747,\n",
       " 'outrage': 748,\n",
       " 'barn': 749,\n",
       " 'smells': 750,\n",
       " 'wash': 751,\n",
       " 'ploughed': 752,\n",
       " 'ground': 753,\n",
       " 'spoiled': 754,\n",
       " 'uncertain': 755,\n",
       " 'coasting': 756,\n",
       " 'child': 757,\n",
       " 'nothings': 758,\n",
       " 'good': 759,\n",
       " 'say': 760,\n",
       " 'pays': 761,\n",
       " 'papered': 762,\n",
       " 'bones': 763,\n",
       " 'reason': 764,\n",
       " 'hes': 765,\n",
       " 'after': 766,\n",
       " 'open': 767,\n",
       " 'outdoors': 768,\n",
       " 'without': 769,\n",
       " 'jerk': 770,\n",
       " 'twitch': 771,\n",
       " 'worlds': 772,\n",
       " 'given': 773,\n",
       " 'wishing': 774,\n",
       " 'passed': 775,\n",
       " 'watchman': 776,\n",
       " 'beat': 777,\n",
       " 'warn': 778,\n",
       " 'moon': 779,\n",
       " 'wildly': 780,\n",
       " 'somewhere': 781,\n",
       " 'ages': 782,\n",
       " 'hence': 783,\n",
       " 'wrong': 784,\n",
       " 't': 785,\n",
       " 'awake': 786,\n",
       " 'tis': 787,\n",
       " 'symbol': 788,\n",
       " 'token': 789,\n",
       " '2o3': 790,\n",
       " 'gained': 791,\n",
       " 'foothold': 792,\n",
       " 'pursued': 793,\n",
       " 'toffile': 794,\n",
       " 'agreed': 795,\n",
       " 'sure': 796,\n",
       " 'senescent': 797,\n",
       " 'reeled': 798,\n",
       " 'lurched': 799,\n",
       " 'bobbed': 800,\n",
       " 'checked': 801,\n",
       " 'piano': 802,\n",
       " 'loudly': 803,\n",
       " 'playing': 804,\n",
       " 'starlight': 805,\n",
       " 'pall': 806,\n",
       " 'keep': 807,\n",
       " 'track': 808,\n",
       " 'peoples': 809,\n",
       " 'daughters': 810,\n",
       " 'hail': 811,\n",
       " 'presence': 812,\n",
       " 'unembodied': 813,\n",
       " 'essence': 814,\n",
       " 'fingerbone': 815,\n",
       " 'wanted': 816,\n",
       " 'lyre': 817,\n",
       " 'within': 818,\n",
       " 'sky': 819,\n",
       " 'neednt': 820,\n",
       " 'think': 821,\n",
       " 'thoughts': 822,\n",
       " 'entombed': 823,\n",
       " 'hopes': 824,\n",
       " 'astartes': 825,\n",
       " 'bediamonded': 826,\n",
       " 'crescent': 827,\n",
       " 'legitimately': 828,\n",
       " 'demand': 829,\n",
       " 'headed': 830,\n",
       " 'for—': 831,\n",
       " 'beside': 832,\n",
       " 'wall': 833,\n",
       " 'stands': 834,\n",
       " 'gilt': 835,\n",
       " 'wouldnt': 836,\n",
       " 'nebulous': 837,\n",
       " 'lustre': 838,\n",
       " 'born': 839,\n",
       " 'end': 840,\n",
       " 'liquescent': 841,\n",
       " 'deal': 842,\n",
       " 'bad': 843,\n",
       " 'masonry': 844,\n",
       " 'thing': 845,\n",
       " 'did': 846,\n",
       " 'destruction': 847,\n",
       " 'ice': 848,\n",
       " 'bear': 849,\n",
       " 'mind': 850,\n",
       " 'enchanted': 851,\n",
       " 'lettered': 852,\n",
       " 'fine': 853,\n",
       " 'fibrils': 854,\n",
       " 'life': 855,\n",
       " 'loved': 856,\n",
       " 'object': 857,\n",
       " 'tear': 858,\n",
       " 'lid': 859,\n",
       " 'nine': 860,\n",
       " 'removed': 861,\n",
       " 'correct': 862,\n",
       " 'err': 863,\n",
       " 'looked': 864,\n",
       " 'saved': 865,\n",
       " 'sold': 866,\n",
       " 'farm': 867,\n",
       " 'walked': 868,\n",
       " 'sort': 869,\n",
       " 'pearly': 870,\n",
       " 'crowd': 871,\n",
       " 'around': 872,\n",
       " 'earthly': 873,\n",
       " 'solemn': 874,\n",
       " 'peals': 875,\n",
       " 'office': 876,\n",
       " 'illumine': 877,\n",
       " 'enkindle': 878,\n",
       " 'dwelling': 879,\n",
       " 'duty': 880,\n",
       " 'swept': 881,\n",
       " 'heavens': 882,\n",
       " 'turning': 883,\n",
       " 'heel': 884,\n",
       " 'wake': 885,\n",
       " 'sigh': 886,\n",
       " 'slight': 887,\n",
       " 'felt': 888,\n",
       " 'tug': 889,\n",
       " 'rather': 890,\n",
       " 'tip': 891,\n",
       " 'table': 892,\n",
       " 'quenching': 893,\n",
       " 'fires': 894,\n",
       " 'ashes': 895,\n",
       " 'hide': 896,\n",
       " 'nighttime': 897,\n",
       " 'downstairs': 898,\n",
       " 'lead': 899,\n",
       " 'aright': 900,\n",
       " 'doesnt': 901,\n",
       " 'seem': 902,\n",
       " 'courage': 903,\n",
       " 'toward': 904,\n",
       " 'blue': 905,\n",
       " 'graveyard': 906,\n",
       " 'marble': 907,\n",
       " 'sculpture': 908,\n",
       " 'furthest': 909,\n",
       " 'bodies': 910,\n",
       " 'sinners': 911,\n",
       " 'sacrifice': 912,\n",
       " 'ripples': 913,\n",
       " 'curl': 914,\n",
       " 'doth': 915,\n",
       " 'pass': 916,\n",
       " 'th': 917,\n",
       " 'expanding': 918,\n",
       " 'eye': 919,\n",
       " 'youd': 920,\n",
       " 'stone': 921,\n",
       " 'baptismal': 922,\n",
       " 'font': 923,\n",
       " 'find': 924,\n",
       " 'cannot': 925,\n",
       " 'themselves': 926,\n",
       " 'wonted': 927,\n",
       " 'faces': 928,\n",
       " 'roses': 929,\n",
       " 'anyone': 930,\n",
       " 'ledges': 931,\n",
       " 'lines': 932,\n",
       " 'ruled': 933,\n",
       " 'southeastnorthwest': 934,\n",
       " 'together': 935,\n",
       " 'pull': 936,\n",
       " 'apart': 937,\n",
       " 'desolately': 938,\n",
       " 'fall': 939,\n",
       " 'strung': 940,\n",
       " 'hair': 941,\n",
       " 'hurt': 942,\n",
       " 'tonight': 943,\n",
       " 'theyve': 944,\n",
       " 'law': 945,\n",
       " 'courts': 946,\n",
       " 'once': 947,\n",
       " 'legended': 948,\n",
       " 'tomb': 949,\n",
       " 'yourself': 950,\n",
       " 'cheering': 951,\n",
       " 'poor': 952,\n",
       " 'dear': 953,\n",
       " 'great': 954,\n",
       " 'wouldst': 955,\n",
       " 'theres': 956,\n",
       " 'simple': 957,\n",
       " 'loss': 958,\n",
       " 'fault': 959,\n",
       " 'husband': 960,\n",
       " 'houses': 961,\n",
       " 'leaf': 962,\n",
       " 'lingered': 963,\n",
       " 'brown': 964,\n",
       " 'best': 965,\n",
       " 'bard': 966,\n",
       " 'because': 967,\n",
       " 'wisest': 968,\n",
       " 'travellers': 969,\n",
       " 'palace': 970,\n",
       " 'tower': 971,\n",
       " 'past': 972,\n",
       " 'rid': 973,\n",
       " 'books': 974,\n",
       " 'guess': 975,\n",
       " 'theyd': 976,\n",
       " 'better': 977,\n",
       " 'drag': 978,\n",
       " 'sunshine': 979,\n",
       " 'sudden': 980,\n",
       " 'movement': 981,\n",
       " 'bodice': 982,\n",
       " 'descent': 983,\n",
       " 'right': 984,\n",
       " 'killed': 985,\n",
       " 'instead': 986,\n",
       " 'field': 987,\n",
       " 'timeeaten': 988,\n",
       " 'towers': 989,\n",
       " 'tremble': 990,\n",
       " 'nature': 991,\n",
       " 'please': 992,\n",
       " 'kitchen': 993,\n",
       " 'chimney': 994,\n",
       " 'journeyed': 995,\n",
       " 'seven': 996,\n",
       " 'world': 997,\n",
       " 'ways': 998,\n",
       " 'growing': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word to index mapping we are gonna create two lists that will contain more lists. Each one of the lists contained in the general listas will be the index representation of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_as_int = []\n",
    "test_text_as_int = []\n",
    "\n",
    "\n",
    "for sentence in train_text:\n",
    "    tokens = sentence.split()\n",
    "    line_as_int = [word_to_index_mapping[token] for token in tokens]\n",
    "    train_text_as_int.append(line_as_int)\n",
    "\n",
    "for sentence in test_text:\n",
    "    tokens = sentence.split()\n",
    "    line_as_int = [word_to_index_mapping.get(token, 0) for token in tokens]\n",
    "    test_text_as_int.append(line_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2509"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset as ready as we can it's time to create the model. Remember that, for a markov model we need two things: a state transition matrix and a initial vector matrix. For performance of the computer we're gonna use numpy arrays. AX will be the transitions matrix and piX will be the initial state vectors. NOTE that we're creating a matrix and a vector of ones because if we make the multiplication of something by 0 (let's imagina transitions that never happens which will probbly happen) we will have 0 and we don't want that. This technique is named add one smoothing and, if used, we will have to be aware of it when doing the transitions from counts to probabilities. By now we'll leave it like it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(word_to_index_mapping)\n",
    "\n",
    "A0 = np.ones((V,V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "A1 = np.ones((V,V))\n",
    "pi1 = np.ones (V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to populate the matrix and the vector we iterate through them. for every lines we check if it's the first element of the line and add a 1 to that element in the vector, otherwise we check the precedent element and the element itself in the vector and its coincidence in the matrix we add 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_matrix_and_initial_vector(text_as_int,matrix,vector):\n",
    "    for line in text_as_int:\n",
    "        last_index = None\n",
    "        for state in line:\n",
    "            if last_index is None:\n",
    "                vector[state] +=1\n",
    "            else:\n",
    "                matrix[last_index, state] +=1\n",
    "            last_index = state\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we populate our vectors and matrices for both labels. In order to see what zip is look this url https://www.programiz.com/python-programming/methods/built-in/zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_matrix_and_initial_vector([line for line, label in zip(train_text_as_int, Ytrain) if label == 0], A0, pi0)\n",
    "populate_matrix_and_initial_vector([line for line, label in zip(train_text_as_int, Ytrain) if label == 1], A1, pi1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier we need to be aware that we've used the add one smoothing so what we do is, for each row of the matrix as well as for the vector, we add all the numbers and divide each number of the row for the sum. Doing that we will obtain the probabilities. After that we use the log in order to have values less close to 0 and avoid that the computer makes an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 /= A0.sum(axis=1, keepdims=True)\n",
    "pi0 /= pi0.sum()\n",
    "\n",
    "A1 /= A1.sum(axis=1, keepdims=True)\n",
    "pi1 /= pi1.sum()\n",
    "\n",
    "logA0 = np.log(A0)\n",
    "logpi0 = np.log(pi0)\n",
    "\n",
    "logA1 = np.log(A1)\n",
    "logpi1 = np.log(pi1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the descompensation of data we need to be aware that the possibility of a line being from Frost is much more strong than the possibility of being from Poe because the dataset contains much more lines from Frost than lines from Poe. In order to calculate this posibility we need to calculate the priors. This prior will need to be taken care of when calculating the probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3461300309597523, 0.6538699690402476)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count0 = sum(y == 0 for y in Ytrain)\n",
    "# Number of total labels in Ytrain from poe\n",
    "count1 = sum(y == 1 for y in Ytrain)\n",
    "# Number of total labels in Ytrain from frost\n",
    "\n",
    "\n",
    "total = len(Ytrain)\n",
    "# Total number of labels\n",
    "\n",
    "probability0 = count0 / total\n",
    "probability1 = count1 / total\n",
    "# probabilities for each author\n",
    "\n",
    "logprobability0 = np.log(probability0)\n",
    "logprobability1 = np.log(probability1)\n",
    "# Log of the rpobabilities\n",
    "probability0,probability1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the probability of a line to be part of poe writtings is just 0.32 while the probability of being of Frost is 0.67. Be aware of that for the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0609407625016578"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobability0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna create a class whose builder method will have a matrix, a vector, the priors and we'll also add length of the priors into its atributes. \n",
    "This class will have 2 methods:\n",
    "\n",
    "-The first method will be named compute_log_likelihood and it would take a matrix and a vector and, using the rules of markov chain, compute the mulitplications. Remember that log(AB) = log(A) + log(B) so we won't have the need to multiply being this operation more demanding for the performance. \n",
    "\n",
    "-The second method is predict and it will do the following. By using the class as well as a suppoused input we will generate a vector that will contain the probability of all the lines from the testing and training set. Each one of the elements in the vector will be a probability then. To the result of adding the log of the probabilities we will also add the log of the priors. So for each model (each initial transitions as well as its matrix) we compute the log likelihood obtaining a list with two elements, one corresponing to each score in the models. Then taking the bigger ones we decide wether if that line is from Frost or from Poe (0 or 1), and we add it to the predictions. predictions is a vector that contains as much elements as lines we have in our dataset, so, in the end, we'll have a vector with all the predictions of the list of inoputs that we've given to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self,logAs,logpis,logpriors):\n",
    "        self.logAs = logAs\n",
    "        self.logpis = logpis\n",
    "        self.logpriors = logpriors\n",
    "        self.Modelos = len(logpriors)\n",
    "\n",
    "    def _compute_log_likelihood(self, input_,class_):\n",
    "        logA = self.logAs[class_]\n",
    "        logpi = self.logpis[class_]\n",
    "\n",
    "        last_state = None\n",
    "        logprob = 0\n",
    "        for state in input_:\n",
    "            if last_state is None:\n",
    "                logprob += logpi[state]\n",
    "            else:\n",
    "                logprob += logA[last_state, state]\n",
    "            last_state = state\n",
    "        return logprob\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        predictions = np.zeros(len(inputs))\n",
    "        for index, input_ in enumerate(inputs):\n",
    "            posteriors = [self._compute_log_likelihood(input_, c) + self.logpriors[c] for c in range(self.Modelos)]\n",
    "            pred = np.argmax(posteriors)\n",
    "            predictions[index] = pred\n",
    "            # np.argmax devuelve el indice del número más alto de la matriz o vector\n",
    "        return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One the class is designed now we declare it with our previous data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier([logA0, logA1],[logpi0,logpi1],[logprobability0,logprobability1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we call predict on both the training set and the test set. Remember that with this we will obtain a vector with the predictions for each line in each set so, by calling the mean over Ytrain that has the labels we will have the accuracy obteined by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.9956656346749226\n"
     ]
    }
   ],
   "source": [
    "Ptrain = clf.predict(train_text_as_int)\n",
    "print(f\"Train acc: {np.mean(Ptrain==Ytrain)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.8367346938775511\n"
     ]
    }
   ],
   "source": [
    "Ptest = clf.predict(test_text_as_int)\n",
    "print(f\"Train acc: {np.mean(Ptest==Ytest)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0201f44e18421a348372a57be9a6ecc536ca8ed47d3c72bd26e81ed63defa3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
